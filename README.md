# Conversational Store Mini-POC

Welcome to the Conversational Store! This project is a mini Proof-of-Concept (POC) demonstrating an intelligent, AI-powered e-commerce experience. Think of it as having a personal shopper built right into the website, ready to help you find exactly what you're looking for.

## Key Features

*   **Interactive Product Storefront:**
    *   Browse a diverse catalog of products (e.g., skincare items across various categories).

*   **Smart Conversational Search:**
    *   Engage in natural conversation to find products. Instead of just typing keywords like "serum," you can say things like "I'm looking for something gentle for my summer skincare routine."
    *   The AI acts as a helpful shopping assistant, asking clarifying follow-up questions to better understand your needs (e.g., "Great! Are you interested in toners, serums, or SPF? Any specific skin concerns or ingredients you'd like to avoid?").
    *   The search interface is designed to be interactive and feel like a continuous conversation, focusing on the most recent exchange.

*   **AI-Powered Recommendations with Justifications:**
    *   Receive product recommendations tailored to your ongoing conversation and stated preferences.
    *   Each recommendation comes with a clear explanation from the AI, so you understand why a particular product is suggested (e.g., "Based on your need for frizzy hair, here are some haircare products that could help...").

*   **Instant Answers to Your Product Questions:**
    *   Ask specific questions about products, such as, "Is this serum suitable for sensitive skin?"
    *   Get informative answers generated by the AI, which draws upon a knowledge base of brand information, customer reviews, and other relevant data. Answers may include direct snippets or citations from these sources.

*   **Intelligent Product Ranking:**
    *   Products are presented to you not just based on relevance, but also considering other factors, such as business logic (e.g., ranking by highest margin while still meeting your needs).

## How It Works (Behind the Scenes)

This Conversational Store leverages cutting-edge AI technologies:

*   **Large Language Models (LLMs):** To understand your queries in natural language and generate human-like responses, questions, and justifications.
*   **Retrieval Augmented Generation (RAG) Pipeline:** This system allows the AI to access and retrieve relevant information from a dedicated knowledge base (including product details, brand information, customer reviews, and support tickets). This ensures the AI's responses are grounded in factual data.
*   The combination of these technologies allows the store to provide a dynamic, interactive, and informative shopping experience.

## Technology Stack

*   **Backend:** Python
*   **Frontend:** TypeScript/React (using Next.js framework)
*   **LLM Integration:** Designed to work with various LLM providers (like OpenAI, Gemini, etc.) with an emphasis on cost-effective inferencing.
*   **Vector Database:** Utilizes a vector index (e.g., FAISS, Chroma) for efficient similarity search within the RAG pipeline.

## Purpose of This Project

This mini-POC serves to demonstrate how a full-stack application can integrate an LLM and a retrieval pipeline to create a sophisticated, conversational, and user-centric e-commerce platform. It highlights thoughtful system orchestration, the incorporation of business logic, and the potential for rich data analytics.

## Getting Started

Follow these steps to set up and run the Conversational Store on your local machine.

### Prerequisites

*   Git
*   Python (3.8 or newer recommended)
*   Node.js (v18 or newer recommended) and npm

### 1. Clone the Repository

```bash
git clone <repository_url>
cd <repository_directory_name>
```
Replace `<repository_url>` with the actual URL of your Git repository and `<repository_directory_name>` with the name of the directory created by cloning.

### 2. Set Up Environment Variables

This project requires API keys and other configurations to be set as environment variables.

*   **Backend (`.env` file):**
    Navigate to the `backend` directory:
    ```bash
    cd backend
    ```
    Create a file named `.env` in the `backend` directory and add the following, replacing placeholder values with your actual credentials and settings:
    ```env
    # backend/.env
    GOOGLE_API_KEY="YOUR_GOOGLE_API_KEY_HERE"
    # Optional: Specify a Gemini model. Defaults to "gemini-2.5-pro-preview-05-06" if not set.
    # GEMINI_MODEL_NAME_E_COM="gemini-1.5-flash-latest" 
    ```
    Navigate back to the project root:
    ```bash
    cd ..
    ```

*   **Frontend (Environment Variables):**
    The frontend primarily communicates with the backend API. The API base URL is configured in `frontend/src/lib/api.ts` and defaults to `http://localhost:8000`. If your backend runs on a different port or host, you might need to adjust this. The `NEXT_PUBLIC_API_BASE_URL` environment variable can be used by the frontend if you set it up in a `.env.local` file in the `frontend` directory.

### 3. Set Up and Run the Backend (Python/FastAPI)

1.  **Navigate to the backend directory:**
    ```bash
    cd backend
    ```

2.  **Create and activate a virtual environment:**
    ```bash
    python -m venv venv
    # On Windows
    # venv\Scripts\activate
    # On macOS/Linux
    source venv/bin/activate
    ```

3.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Ingest data for the RAG pipeline:**
    This step populates the vector database with product information, reviews, etc.
    ```bash
    python ingest_data.py
    ```
    *(Note: Ensure all data files mentioned in `backend/ingest_data.py` like `skincare_catalog.csv`, `brand_info.txt`, `verified_reviews.txt`, and `customer_tickets.txt` are present in the `backend/data/` directory as per `backend/ingest_data.py` configuration.)*

5.  **Run the FastAPI server:**
    ```bash
    uvicorn src.api.main:app --reload --host 0.0.0.0 --port 8000
    ```
    The backend API should now be running at `http://localhost:8000`. You can check its status by opening `http://localhost:8000/docs` in your browser.

### 4. Set Up and Run the Frontend (TypeScript/React/Next.js)

1.  **Navigate to the frontend directory (from the project root):**
    ```bash
    cd frontend
    ```

2.  **Install dependencies:**
    ```bash
    npm install
    ```

3.  **Run the Next.js development server:**
    ```bash
    npm run dev
    ```
    The frontend application should now be running at `http://localhost:3000`.

### 5. Access the Application

Open your web browser and navigate to `http://localhost:3000` to use the Conversational Store.

---

We hope you enjoy exploring the capabilities of the Conversational Store!
